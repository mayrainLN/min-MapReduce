## 实现
### 流式传输/计算

所有文件的读取、大部分计算都是通过Stream流式进行，传输shuffle数据使用netty
服务间通信使用akka，都是长连接。

客户端从服务端拉取数据，为了防止一直拉取，打爆客户端内存，使用了以下方法：客户端网络IO获取到的shuffle，都会存在本地的阻塞队列，计算的时候客户端从阻塞队列中拿。如果计算的很慢，阻塞队列满了，channelRead中的que.add就会阻塞。 计算完一条，再从阻塞队列拉出来一条，阻塞队列空了，客户端才会继续再从服务端拉取shuffle放入阻塞队列，否则会阻塞。阻塞队列很好地和流式计算配合。



### 改造Reduce Task / 创建Merge Task

原本的reduce任务只是统计url数量，输出到output文件。

现在最终目标是UrlTopN，所以我们的reduce结果不再是最终结果，也是一个shuffle文件。

所有reduce的输出数据源放入了Executor本地的shuffle文件夹。

merge任务在一个Executor上进行即可。merge的结果也不必再做一个shuffle文件，直接通过MergeTaskStatus传递给Driver即可。由Driver在本地持久化。

### 文件切分：

- 为了反正url被截断，考虑按照文本内容切分：
  可以用split，比如按照空格或者"去分割
  重大缺点：会改变文件的内容(没有分隔符)。
  我们的分片应当是普遍适用的，可以服务于各种计算任务，不能因为这样切割可以满足TopN的需求就这样搞。
- 如果要保留源文件的所有内容，只考虑这样做：每次读取一个字符，直到当前读取的字符超过了1M，就开始准备切片。为了防止切断完整的url，要求在读取到冒号或者空格或者逗号时，才能将当前所读的所有字符生成一个文件切片。否则，继续向后读取，即使大小超过了1M。这样可以防止url被截断
  缺点：一个个读再判断，实在是太慢了。

 考虑到在答疑文档总老师补充的信息：同一个url不会分布在上下两行。
 所以其实我们可以利用这个信息，**每次读取一行即可。超过1M就生成切片**

### 统一本地测试Case与Thrift PRC提交任务

改造Driver的主类，将所有任务的提交统一放在线程池中做。

抽象出一个Application类实现Runnable。

由于任务结果只能通过ApplicationID查询，所以需要额外记录ApplicationID和Outputpath之间的关系。

### TopN的计算

对于TopN=x的任务，每个reduce，我们只需要取前x个url（最后一名可以重复使得总数>=x）

最后将k个reduce的结果再次合并，就可以得到前x个url。减少资源消耗

先排序，去除第x位置的数。**其实这里可以优化（用快排寻找pivot的方法），无需全量排序**

再次遍历结果，所有>=x的数就是我们需要的结果。





